# ðŸ“ Week [2] â€” [Vector Search]

> ðŸ“… Date: [Insert date]  
> ðŸ” Topic: [LLMs / Vector Search / RAG / etc.]  
> ðŸŽ¯ Goal: [What was this weekâ€™s goal or assignment?]  
Learn about Vector Data bases aand how its works

---

## âœ… Key Concepts

- ðŸ”¹ **Concept 1**: RAG System - Q&A System  
- ðŸ”¹ **Concept 2**: Embedding  
- ðŸ”¹ **Concept 3**: Vector Search (Qdrant)  

> âœ¨ _Use bullet points to highlight insights or aha moments._

---

## ðŸ“ˆ RAG Pipeline â€“ Dataflow Diagram

```mermaid
flowchart TD
    A(["ðŸ“¨ User Question"]) --> B(["ðŸ”Ž Embed Query"])
    B --> C(["ðŸ“ Vector Search (Qdrant)"])
    C --> D(["ðŸ“„ Retrieve Top-K Docs"])
    D --> E(["ðŸ§  Add Context to Prompt"])
    E --> F(["ðŸ¤– LLM (GPT-4)"])
    F --> G(["âœ… Return Answer"])
```
---


## ðŸ’¬ Reflection 

> â“ **Qdrant will be part of a RAG system, Do I need splitting/chunking, or does Qdrant handle that for me?**


### âœ… Soâ€¦ Does Qdrant Create the Semantic Vectors?

> âŒ **No, Qdrant does not embed or chunk your data.**

Qdrant is a **storage and search engine for vectors** â€” it expects **you to provide the vectors**, and optionally some metadata (like the original text, tags, document ID, etc.).

You are still responsible for:

1. ðŸ”¹ **Splitting the input text into chunks**
2. ðŸ”¹ **Embedding each chunk** using a model (e.g. OpenAI, HuggingFace,  FastEmbed as our embedding provider)
3. ðŸ”¹ **Sending each vector into Qdrant as a point**

Only then Qdrant can perform **semantic search**.

---

### ðŸ§© Why is Splitting/Chunking Important?

- LLMs and embedding models have token limits (e.g. 512â€“2048 tokens).
- Embedding an entire book or long document often loses **semantic precision**.
- Chunking enables you to **preserve context boundaries**, such as paragraphs or topics.
- Smaller chunks â†’ more precise matches â†’ better answers from the LLM.

---

### ðŸ“¦ What Qdrant Actually Handles

- Fast nearest-neighbor search in vector space (HNSW, filtering, etc.)
- Metadata filtering (e.g. search only inside `type="faq"` documents)
- Storing IDs, payloads, and versions of your documents
- Optional vector compression or quantization (advanced use)

---

### ðŸ” Typical Flow with Qdrant in a RAG System

```text
[Document Text]
    â†“  (Chunking)
[Chunks of 200â€“500 words]
    â†“  (Embedding - FastEmbed)
[Vector Representations]
    â†“  (Store in Qdrant)
[Qdrant Collection: Vectors + Payloads]
    â†“  (User Query â†’ Embed)
[Vector Search â†’ Top-k Chunks]
    â†“
[LLM Prompt (Query + Context)]
    â†“
[Generated Answer]
```

---

## âœ… Summary Takeaway

> Qdrant **is a powerful semantic search engine**, but **you still need to do the chunking and embedding** before inserting your data. It doesnâ€™t replace that step â€” it enables it at scale.---
> FastEmbed **is an optimized embedding solution** designed specifically for **Qdrant**
> Like most dense embedding models, **jina-embedding-small-en was trained to measure semantic closeness** using **cosine similarity**.
You can find this information, for example, on the modelâ€™s [Hugging Face card.](https://huggingface.co/jinaai/jina-embeddings-v2-small-en)
> The best way to **select an embedding model is to test and benchmark** different options on your **own data**.

---

## ðŸ“š Resources & References

- [Qdrant is an open-source: vector search engine](https://qdrant.tech/articles/dedicated-vector-search/)
- [FastEmbed Embeddings](https://github.com/qdrant/fastembed)
- [ChromaDB GitHub](https://github.com/chroma-core/chroma)
- [Hugging Face card](https://huggingface.co/jinaai/jina-embeddings-v2-small-en)

